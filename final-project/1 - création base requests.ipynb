{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import requests\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from datetime import date\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTRIES = pd.read_csv('pays.csv')\n",
    "LISTE_COUNTRIES = list(COUNTRIES.Pays)\n",
    "\n",
    "SITES_EXCLUS = ['wikipedia', 'vikidia','accounts.google','.pdf']\n",
    "NB_PAGES_GOOGLE = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions\n",
    "- Création de la liste d'URL par pays\n",
    "- Récupération du body des pages\n",
    "- Nettoyage basique de la page : - html, - caractères spéciaux, - URL, passage en minuscules\n",
    "- Plusieurs fonctions de génération des dates suivant le besoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ListeURL(pays,nresult,DateMin, DateMax):\n",
    "    Liste_URL = []\n",
    "    M_Min = str(DateMin.month)\n",
    "    A_Min = str(DateMin.year)\n",
    "    M_Max = str(DateMax.month)\n",
    "    A_Max = str(DateMax.year)\n",
    "    for x in range(nresult):\n",
    "        google = driver.get(\"http://www.google.com/search?q=\"+pays+\"&lr=lang_fr&start=\"+str(x*10)+\"&tbs=cdr%3A1%2Ccd_min%3A\"+M_Min+\"%2F1%2F\"+A_Min+\"%2Ccd_max%3A\"+M_Max+\"%2F11%2F\"+A_Max)\n",
    "        URLs = driver.find_elements_by_xpath(\"//div[@class='r']/a \")\n",
    "        URLs = [x.get_attribute('href') for x in URLs]\n",
    "        URLs = [x for x in URLs if (\"wikipedia\" not in x) & (\"vikidia\" not in x) & (\"accounts.google\" not in x)]\n",
    "        Liste_URL.extend(URLs)\n",
    "    return Liste_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getpage(listeURL,num):\n",
    "    page = requests.get(listeURL[num], timeout = 30).text\n",
    "    soup = BeautifulSoup(page)\n",
    "    body = soup.find('body')\n",
    "    contenu_page = body.get_text()\n",
    "    return (contenu_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_by_regex(txt, regexp):\n",
    "    return re.sub(regexp, '', txt)\n",
    "\n",
    "def remove_url(txt):\n",
    "    return remove_by_regex(txt, re.compile(r\"http.?://[^\\s]+[\\s]?\"))\n",
    "\n",
    "def remove_special_char(txt):\n",
    "    return re.sub(r\"[^a-zA-ZàçéèùïîöôëêäâûüÀÇÉÈÙÏÎÖÔËÊÄÂÛÜœæŒÆ ]\", \" \", unidecode(txt))\n",
    "\n",
    "def clean_up(txt):\n",
    "    txt = remove_url(txt)\n",
    "    txt = remove_special_char(txt)\n",
    "    txt = re.sub('<[^<]+?>', '', txt)\n",
    "    txt = re.sub('\\n', ' ', txt)\n",
    "    txt = re.sub('\\t', ' ', txt)\n",
    "    return txt.lower().strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def liste_dates_trim(AnneeMin,AnneeMax):\n",
    "    MoisMin = [\"01-01\",\"01-04\",\"01-07\",\"01-10\"]\n",
    "    MoisMax = [\"31-03\",\"30-06\",\"30-09\",\"31-12\"]\n",
    "    Annees = range(AnneeMin,AnneeMax+1)\n",
    "    liste_date_min = []\n",
    "    liste_date_max = []\n",
    "    for x in Annees:\n",
    "        for y in range(len(MoisMin)):\n",
    "            liste_date_min.append(MoisMin[y]+'-'+str(x))\n",
    "            liste_date_max.append(MoisMax[y]+'-'+str(x))\n",
    "    return liste_date_min, liste_date_max\n",
    "\n",
    "\n",
    "def liste_dates_an(AnneeMin,AnneeMax):\n",
    "    Annees = range(AnneeMin,AnneeMax+1)\n",
    "    liste_date_min = []\n",
    "    liste_date_max = []\n",
    "    for x in Annees:\n",
    "        liste_date_min.append(\"01-01\"+'-'+str(x))\n",
    "        liste_date_max.append(\"31-12\"+'-'+str(x))\n",
    "    return liste_date_min, liste_date_max\n",
    "\n",
    "def liste_dates_ans_separes(Annees):\n",
    "    liste_date_min = []\n",
    "    liste_date_max = []\n",
    "    for x in Annees:\n",
    "        liste_date_min.append(\"01-01\"+'-'+str(x))\n",
    "        liste_date_max.append(\"31-12\"+'-'+str(x))\n",
    "    return liste_date_min, liste_date_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialisation de la base de résultat + ouverture du navigateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = pd.DataFrame(columns = ['Date_debut','Date_fin','Pays', 'Rang','Liste_URL', 'Contenu_page'])\n",
    "driver = webdriver.Chrome(r'C:\\Users\\lleki\\chromedriver.exe')\n",
    "driver.maximize_window()\n",
    "dates_min, dates_max = liste_dates_ans_separes([2005])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gestions des exceptions\n",
    "Permet d'ajuster le code pour un périmètre pays et/ou période non prévu (notamment pour compléter une requête interrompue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dates_min_select = dates_min[0]\n",
    "#dates_max_select = dates_max[0]\n",
    "#dates_min = []\n",
    "#dates_max = []\n",
    "#dates_min.append(dates_min_select)\n",
    "#dates_max.append(dates_max_select)\n",
    "#\n",
    "dern_pays = \"Ouganda\"\n",
    "rang_der_pays = COUNTRIES[COUNTRIES.Pays == dern_pays]\n",
    "LISTE_COUNTRIES = LISTE_COUNTRIES[LISTE_COUNTRIES.index(dern_pays)+1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boucle de constitution de la base\n",
    "- définit la liste de périodes / de pays sur lesquels requêter\n",
    "- identifie les URL\n",
    "- récupère les contenus des pages\n",
    "- sauvegarde au fur et à mesure\n",
    "- ferme le navigateur après avoir terminé le process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Afghanistan0\n",
      "Afghanistan1\n",
      "Afghanistan2\n",
      "Afghanistan3\n",
      "Afghanistan4\n",
      "Afghanistan5\n",
      "Afghanistan6\n",
      "Afghanistan7\n",
      "Afghanistan8\n",
      "Afghanistan9\n",
      "Afghanistan10\n",
      "Afghanistan11\n",
      "Afghanistan12\n",
      "Afghanistan13\n",
      "Afghanistan14\n",
      "Afghanistan15\n",
      "Afghanistan16\n",
      "Afghanistan17\n",
      "Afghanistan18\n",
      "Afghanistan19\n",
      "Afghanistan20\n",
      "Afghanistan21\n",
      "Afghanistan22\n",
      "Afghanistan23\n",
      "Afghanistan24\n",
      "Afghanistan25\n",
      "Afghanistan26\n",
      "Afghanistan27\n",
      "Afghanistan01-01-2005/31-12-2005\n"
     ]
    }
   ],
   "source": [
    "for d in range(len(dates_min)):\n",
    "    DateMin_asdate = datetime.strptime(dates_min[d], '%d-%m-%Y')\n",
    "    DateMax_asdate = datetime.strptime(dates_max[d], '%d-%m-%Y')\n",
    "    liste_lignes = []\n",
    "    sites_bugs = []\n",
    "    for pays in LISTE_COUNTRIES:\n",
    "        listeURL = ListeURL(pays, NB_PAGES_GOOGLE,DateMin_asdate, DateMax_asdate)\n",
    "        for x in range(len(listeURL)):\n",
    "            try:\n",
    "                contenu_page = getpage(listeURL,x)\n",
    "                clean_page = clean_up(contenu_page)\n",
    "                ligne_base = [DateMin_asdate,DateMax_asdate, pays, x, listeURL[x], clean_page]\n",
    "                liste_lignes.append(ligne_base)\n",
    "                pd.DataFrame(liste_lignes).to_csv('lignes_en_cours.csv')\n",
    "                print(pays+str(x))\n",
    "            except:\n",
    "                sites_bugs.append(listeURL[x])\n",
    "                pd.DataFrame(sites_bugs).to_csv('sites_bugs.csv')\n",
    "                print(\"pb sur l'URL\" + listeURL[x])\n",
    "        print(pays+dates_min[d]+\"/\"+dates_max[d])\n",
    "        pd.DataFrame(liste_lignes).to_csv('dernier_pays_enregistré.csv')\n",
    "    base = pd.DataFrame.from_records(liste_lignes)\n",
    "    base.to_csv('base_résultats_'+str(DateMin_asdate.month)+\"-\"+str(DateMin_asdate.year)+'_'+str(DateMax_asdate.month)+\"-\"+str(DateMax_asdate.year)+'.csv')\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
